U radu:

Oni su uzeli klasični VGG19 (19 slojeva: 16 konvolucijskih + 3 FC) i modifikovali ga da bolje radi na FER2013 i CK+ datasetima, jer je običan VGG19 sklon overfittingu i previše težak za male datasete.
“Improved VGG19” = verzija sa unapređenjima:

Unutar svakog bloka:

Conv sloj (za osobine)

BatchNorm (ubrzava treniranje, stabilizuje)

ReLU (nelinearnost)

Average pooling (umesto MaxPooling, da uhvati globalnije feature-e)

Dropout

Između poslednjeg convolution sloja i FC sloja → smanjuje overfitting.

Fully Connected slojevi

Umesto 3 teška FC sloja iz originalnog VGG19, oni koriste samo 2 FC sloja + softmax → lakši, stabilniji, manje parametara.

Loss funkcija

Koriste cross-entropy + softmax (bolja za multi-class, otpornija na noisy labels) umesto klasične MSE.


additional fine tuning: sa class weights za fear surpsrise i angry, popravio malo, ali neke pokvario, import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, metrics
from tensorflow.keras.applications import VGG19
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# --- Učitavanje prethodno treniranog modela ---
best_model = tf.keras.models.load_model("best_vgg19_model.keras")

# --- Parametri ---
fine_tune_epochs = 20
batch_size = 128

# --- Postavljanje slojeva za fine-tuning ---
vgg = best_model.get_layer("vgg19")
fine_tune_at = 12  # block4_conv1

for layer in vgg.layers[:fine_tune_at]:
    layer.trainable = False
for layer in vgg.layers[fine_tune_at:]:
    if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):
        layer.trainable = True
    else:
        layer.trainable = False

# --- Optimizator i callback-ovi ---
fine_tune_optimizer = optimizers.Adam(learning_rate=1e-5, amsgrad=True)

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=6,
    restore_best_weights=True,
    verbose=1
)

lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-8,
    verbose=1
)

model_checkpoint = ModelCheckpoint(
    'best_vgg19_model.keras',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

best_model.compile(
    optimizer=fine_tune_optimizer,
    loss='categorical_crossentropy',
    metrics=[
        'accuracy',
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall'),
        tf.keras.metrics.AUC(name='auc')
    ]
)



train_class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
problematic_class_names = ['Angry', 'Fear', 'Surprise']  # klase sa slabijim rezultatom

num_classes = len(train_class_names)
class_weights = np.ones(num_classes, dtype=np.float32)

# Postavljanje tezina za problematicne klase
for cls_name in problematic_class_names:
    idx = train_class_names.index(cls_name)
    if cls_name == 'Fear':
        class_weights[idx] = 3.0   # Fear = 3
    else:
        class_weights[idx] = 2.0   # Angry i Surprise = 2


print("class_weights:", class_weights)

# Funkcija za dodavanje sample_weights 
def add_sample_weights(x, y):
    sample_weight = tf.reduce_max(y * class_weights)
    return x, y, sample_weight

train_ds_for_fit = train_ds_rgb_1.map(add_sample_weights, num_parallel_calls=tf.data.AUTOTUNE)

# Fine-tuning sa sample_weights
history_fine = best_model.fit(
    train_ds_for_fit,
    validation_data=val_ds_rgb_1,
    epochs=fine_tune_epochs,
    initial_epoch=0,
    callbacks=[early_stop, lr_scheduler, model_checkpoint]
)

# --- Evaluacija ---
best_model = tf.keras.models.load_model('best_vgg19_model.keras')
eval_results = best_model.evaluate(test_ds_rgb_1)
print("Test results (loss, accuracy, precision, recall, AUC):", eval_results)

# --- Predikcija i metrike ---
y_true = np.concatenate([y for x, y in test_ds_rgb_1], axis=0)
y_pred_prob = best_model.predict(test_ds_rgb_1)
y_pred = np.argmax(y_pred_prob, axis=1)
y_true_classes = np.argmax(y_true, axis=1)
nije specijalna razlika, precision osciluje, nema veceg napretka

ovo se dobije:
=== Eval results (from model.evaluate - loss, accuracy, precision, recall, AUC) ===
[1.1010960340499878, 0.5832154154777527, 0.7483461499214172, 0.4002264440059662, 0.898094117641449]

=== Global metrics ===
Accuracy: 0.5832
Macro Precision: 0.5301
Weighted Precision: 0.5792
Macro Recall: 0.5733
Weighted Recall: 0.5832
Macro F1: 0.5381
Weighted F1: 0.5773
AUC (OVR): 0.8769

=== Per-class metrics ===
              precision    recall  f1-score   support

           0     0.4893    0.4542    0.4711       960
           1     0.3086    0.7117    0.4305       111
           2     0.4226    0.3350    0.3737      1018
           3     0.7753    0.8526    0.8121      1825
           4     0.5067    0.5938    0.5468      1216
           5     0.4795    0.4012    0.4369      1139
           6     0.7290    0.6650    0.6955       797

    accuracy                         0.5832      7066
   macro avg     0.5301    0.5733    0.5381      7066
weighted avg     0.5792    0.5832    0.5773      7066
